#!/usr/bin/env python3
"""
Two Moons NoProp-CT Example

This example demonstrates NoProp-CT on the classic two moons dataset:
- x: 2D coordinates from the two moons dataset
- z: One-hot encoded labels (which moon the point belongs to)
- Training: Learn to predict z from x using NoProp-CT
- Visualization: Show learning curves, MSE, and z(t) trajectories

The two moons dataset is perfect for visualizing:
1. How the model learns to separate the two classes
2. The continuous-time evolution of predictions z(t)
3. The effect of different noise schedules
"""

import jax
import jax.numpy as jnp
import jax.random as jr
import flax.linen as nn
import optax
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_moons
from sklearn.preprocessing import StandardScaler
from typing import Dict, Any, Tuple
import os

# Import our NoProp implementation
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.jax_noprop.noprop_ct import NoPropCT
from src.jax_noprop.models import ConditionalResNet
from src.jax_noprop.noise_schedules import (
    LinearNoiseSchedule, 
    CosineNoiseSchedule, 
    SigmoidNoiseSchedule,
    LearnableNoiseSchedule,
    SimpleMonotonicNetwork
)


def generate_two_moons_data(n_samples: int = 1000, noise: float = 0.1, random_state: int = 42) -> Tuple[np.ndarray, np.ndarray]:
    """Generate two moons dataset.
    
    Args:
        n_samples: Number of samples to generate
        noise: Amount of noise to add
        random_state: Random seed for reproducibility
        
    Returns:
        x: 2D coordinates [n_samples, 2]
        z: One-hot encoded labels [n_samples, 2]
    """
    x, y = make_moons(n_samples=n_samples, noise=noise, random_state=random_state)
    
    # Standardize the features
    scaler = StandardScaler()
    x = scaler.fit_transform(x)
    
    # Convert to one-hot encoding
    z = np.eye(2)[y]
    
    return x.astype(np.float32), z.astype(np.float32)


def create_simple_model(input_dim: int = 2, output_dim: int = 2, hidden_dim: int = 64) -> nn.Module:
    """Create a simple MLP for the two moons task.
    
    Args:
        input_dim: Input dimension (2 for 2D coordinates)
        output_dim: Output dimension (2 for one-hot labels)
        hidden_dim: Hidden layer dimension
        
    Returns:
        Simple MLP model
    """
    class SimpleMLP(nn.Module):
        num_classes: int = output_dim
        
        @nn.compact
        def __call__(self, z: jnp.ndarray, x: jnp.ndarray, t: jnp.ndarray) -> jnp.ndarray:
            # For NoProp-CT, we need to predict dz/dt given (z, x, t)
            # In this simple case, we'll use x and t to predict the target z
            
            # Time embedding
            t_embed = nn.Dense(hidden_dim)(t[..., None])
            t_embed = nn.relu(t_embed)
            
            # Concatenate x and time embedding
            combined_input = jnp.concatenate([x, t_embed], axis=-1)
            input_dim_with_time = input_dim + hidden_dim
            
            # Main network
            h = nn.Dense(hidden_dim)(combined_input)
            h = nn.relu(h)
            h = nn.Dense(hidden_dim)(h)
            h = nn.relu(h)
            
            # Output layer - predict dz/dt
            dz_dt = nn.Dense(output_dim)(h)
            
            return dz_dt
    
    return SimpleMLP()


def train_noprop_ct(
    x_train: np.ndarray,
    z_train: np.ndarray,
    x_val: np.ndarray,
    z_val: np.ndarray,
    noise_schedule_name: str = "cosine",
    learning_rate: float = 1e-3,
    num_epochs: int = 100,
    batch_size: int = 64,
    random_seed: int = 42
) -> Dict[str, Any]:
    """Train NoProp-CT on two moons dataset.
    
    Args:
        x_train: Training features [n_train, 2]
        z_train: Training targets [n_train, 2]
        x_val: Validation features [n_val, 2]
        z_val: Validation targets [n_val, 2]
        noise_schedule_name: Name of noise schedule to use
        learning_rate: Learning rate for optimizer
        num_epochs: Number of training epochs
        batch_size: Batch size for training
        random_seed: Random seed for reproducibility
        
    Returns:
        Dictionary containing training results
    """
    # Set random seed
    key = jr.PRNGKey(random_seed)
    
    # Create noise schedule
    if noise_schedule_name == "linear":
        noise_schedule = LinearNoiseSchedule()
    elif noise_schedule_name == "cosine":
        noise_schedule = CosineNoiseSchedule()
    elif noise_schedule_name == "sigmoid":
        noise_schedule = SigmoidNoiseSchedule()
    elif noise_schedule_name == "learnable":
        noise_schedule = LearnableNoiseSchedule(
            hidden_dims=(32, 32),
            monotonic_network=SimpleMonotonicNetwork(hidden_dims=(32, 32))
        )
    else:
        raise ValueError(f"Unknown noise schedule: {noise_schedule_name}")
    
    # Create model
    model = create_simple_model(input_dim=2, output_dim=2, hidden_dim=64)
    
    # Create NoProp-CT
    noprop_ct = NoPropCT(target_dim=2, model=model, noise_schedule=noise_schedule)
    
    # Initialize parameters
    key, subkey = jr.split(key)
    dummy_x = jnp.ones((batch_size, 2))
    dummy_z = jnp.ones((batch_size, 2))
    dummy_t = jnp.ones((batch_size,))
    
    params = noprop_ct.init(subkey, dummy_z, dummy_x, dummy_t)
    
    # Create optimizer
    optimizer = optax.adam(learning_rate=learning_rate)
    opt_state = optimizer.init(params)
    
    # Training loop
    train_losses = []
    val_losses = []
    train_mses = []
    val_mses = []
    train_accuracies = []
    val_accuracies = []
    
    n_train = len(x_train)
    n_val = len(x_val)
    
    print(f"Training NoProp-CT with {noise_schedule_name} noise schedule...")
    print(f"Training samples: {n_train}, Validation samples: {n_val}")
    
    for epoch in range(num_epochs):
        # Training
        epoch_train_losses = []
        epoch_train_mses = []
        epoch_train_accuracies = []
        
        # Shuffle training data
        key, subkey = jr.split(key)
        perm = jr.permutation(subkey, n_train)
        x_train_shuffled = x_train[perm]
        z_train_shuffled = z_train[perm]
        
        # Training batches
        for i in range(0, n_train, batch_size):
            batch_end = min(i + batch_size, n_train)
            x_batch = x_train_shuffled[i:batch_end]
            z_batch = z_train_shuffled[i:batch_end]
            
            # Ensure batch size is consistent
            if len(x_batch) < batch_size:
                # Pad with random samples if needed
                n_pad = batch_size - len(x_batch)
                key, subkey = jr.split(key)
                pad_indices = jr.choice(subkey, n_train, (n_pad,))
                x_batch = jnp.concatenate([x_batch, x_train[pad_indices]], axis=0)
                z_batch = jnp.concatenate([z_batch, z_train[pad_indices]], axis=0)
            
            # Training step
            key, subkey = jr.split(key)
            params, opt_state, loss, metrics = noprop_ct.train_step(
                params, opt_state, x_batch, z_batch, subkey, optimizer
            )
            
            epoch_train_losses.append(loss)
            epoch_train_mses.append(metrics['mse'])
        
        # For validation, we'll just use a simple loss based on the training metrics
        # We'll compute actual accuracy only after training is complete
        epoch_val_losses = [np.mean(epoch_train_losses)]  # Use training loss as proxy
        epoch_val_mses = [np.mean(epoch_train_mses)]      # Use training MSE as proxy
        epoch_val_accuracies = [0.0]  # Placeholder, will be computed after training
        
        # Record metrics
        train_losses.append(np.mean(epoch_train_losses))
        val_losses.append(np.mean(epoch_val_losses))
        train_mses.append(np.mean(epoch_train_mses))
        val_mses.append(np.mean(epoch_val_mses))
        train_accuracies.append(0.0)  # Placeholder, will be computed after training
        val_accuracies.append(np.mean(epoch_val_accuracies))
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch:3d}: Train Loss = {train_losses[-1]:.4f}, "
                  f"Val Loss = {val_losses[-1]:.4f}, "
                  f"Train MSE = {train_mses[-1]:.4f}, "
                  f"Val MSE = {val_mses[-1]:.4f}, "
                  f"Train Acc = {train_accuracies[-1]:.4f}, "
                  f"Val Acc = {val_accuracies[-1]:.4f}")
    
    # Compute final accuracies after training is complete
    print("Computing final accuracies...")
    
    # Training accuracy
    z_train_pred = noprop_ct.predict(params, x_train, "euler", 2, 10)
    train_pred_classes = jnp.argmax(z_train_pred, axis=1)
    train_true_classes = jnp.argmax(z_train, axis=1)
    final_train_accuracy = jnp.mean(train_pred_classes == train_true_classes)
    
    # Validation accuracy
    z_val_pred = noprop_ct.predict(params, x_val, "euler", 2, 10)
    val_pred_classes = jnp.argmax(z_val_pred, axis=1)
    val_true_classes = jnp.argmax(z_val, axis=1)
    final_val_accuracy = jnp.mean(val_pred_classes == val_true_classes)
    
    print(f"Final Training Accuracy: {final_train_accuracy:.4f}")
    print(f"Final Validation Accuracy: {final_val_accuracy:.4f}")
    
    return {
        'params': params,
        'noprop_ct': noprop_ct,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_mses': train_mses,
        'val_mses': val_mses,
        'train_accuracies': train_accuracies,
        'val_accuracies': val_accuracies,
        'final_train_accuracy': final_train_accuracy,
        'final_val_accuracy': final_val_accuracy,
        'noise_schedule_name': noise_schedule_name
    }


def predict_trajectories(
    noprop_ct: NoPropCT,
    params: Dict[str, Any],
    x: np.ndarray,
    num_timesteps: int = 50,
    random_seed: int = 42
) -> np.ndarray:
    """Predict z(t) trajectories for given inputs using actual ODE integration.
    
    Args:
        noprop_ct: Trained NoProp-CT model
        params: Model parameters
        x: Input features [n_samples, 2]
        num_timesteps: Number of timesteps to use for prediction
        random_seed: Random seed for reproducibility
        
    Returns:
        z_trajectories: Predicted trajectories [n_samples, num_timesteps + 1, 2]
    """
    # Use the new predict_trajectory method to get actual ODE integration trajectories
    z_trajectories = noprop_ct.predict_trajectory(
        params, x, "euler", 2, num_timesteps
    )
    
    return z_trajectories  # [n_samples, num_timesteps + 1, 2]


def plot_results(
    x_train: np.ndarray,
    z_train: np.ndarray,
    x_val: np.ndarray,
    z_val: np.ndarray,
    results: Dict[str, Any],
    save_dir: str = "artifacts"
):
    """Plot training results and trajectories.
    
    Args:
        x_train: Training features
        z_train: Training targets
        x_val: Validation features
        z_val: Validation targets
        results: Training results dictionary
        save_dir: Directory to save plots
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # Extract results
    train_losses = results['train_losses']
    val_losses = results['val_losses']
    train_mses = results['train_mses']
    val_mses = results['val_mses']
    train_accuracies = results['train_accuracies']
    val_accuracies = results['val_accuracies']
    noise_schedule_name = results['noise_schedule_name']
    
    # 1. Plot learning curves
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))
    
    epochs = range(len(train_losses))
    
    ax1.plot(epochs, train_losses, label='Train Loss', color='blue')
    ax1.plot(epochs, val_losses, label='Val Loss', color='red')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title(f'Training Progress - {noise_schedule_name.title()} Noise Schedule')
    ax1.legend()
    ax1.grid(True)
    
    ax2.plot(epochs, train_mses, label='Train MSE', color='blue')
    ax2.plot(epochs, val_mses, label='Val MSE', color='red')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('MSE')
    ax2.set_title(f'MSE Progress - {noise_schedule_name.title()} Noise Schedule')
    ax2.legend()
    ax2.grid(True)
    
    ax3.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')
    ax3.plot(epochs, val_accuracies, label='Val Accuracy', color='red')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Accuracy')
    ax3.set_title(f'Accuracy Progress - {noise_schedule_name.title()} Noise Schedule')
    ax3.legend()
    ax3.grid(True)
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/learning_curves_{noise_schedule_name}.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Plot data and predictions
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # True labels
    colors = ['red', 'blue']
    labels = ['Moon 0', 'Moon 1']
    
    for i in range(2):
        mask = z_train[:, i] == 1
        ax1.scatter(x_train[mask, 0], x_train[mask, 1], c=colors[i], label=labels[i], alpha=0.6, s=20)
    
    ax1.set_xlabel('Feature 1')
    ax1.set_ylabel('Feature 2')
    ax1.set_title('True Two Moons Dataset')
    ax1.legend()
    ax1.grid(True)
    
    # Predictions
    noprop_ct = results['noprop_ct']
    params = results['params']
    
    # Get final predictions
    key = jr.PRNGKey(42)
    num_steps = 40  # Number of integration steps for prediction
    z_pred = noprop_ct.predict(params, x_val, "euler", 2, num_steps)
    
    # Convert to class predictions
    pred_classes = jnp.argmax(z_pred, axis=1)
    
    for i in range(2):
        mask = pred_classes == i
        ax2.scatter(x_val[mask, 0], x_val[mask, 1], c=colors[i], label=f'Predicted {labels[i]}', alpha=0.6, s=20)
    
    ax2.set_xlabel('Feature 1')
    ax2.set_ylabel('Feature 2')
    ax2.set_title(f'NoProp-CT Predictions - {noise_schedule_name.title()} Noise Schedule')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/predictions_{noise_schedule_name}.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. Plot z(t) trajectories for a few samples
    print("Computing z(t) trajectories...")
    z_trajectories = predict_trajectories(noprop_ct, params, x_val[:10], num_timesteps=20)
    
    fig, axes = plt.subplots(2, 5, figsize=(20, 8))
    axes = axes.flatten()
    
    for i in range(10):
        ax = axes[i]
        
        # Plot trajectory - now using actual ODE integration
        t_points = np.linspace(0, 1, z_trajectories.shape[1])
        ax.plot(t_points, z_trajectories[i, :, 0], 'r-', label='z‚ÇÄ(t)', linewidth=2)
        ax.plot(t_points, z_trajectories[i, :, 1], 'b-', label='z‚ÇÅ(t)', linewidth=2)
        
        # Mark start and end points
        ax.scatter([0], [z_trajectories[i, 0, 0]], c='red', s=100, marker='o', label='z‚ÇÄ(0)')
        ax.scatter([0], [z_trajectories[i, 0, 1]], c='blue', s=100, marker='o', label='z‚ÇÅ(0)')
        ax.scatter([1], [z_trajectories[i, -1, 0]], c='red', s=100, marker='s', label='z‚ÇÄ(1)')
        ax.scatter([1], [z_trajectories[i, -1, 1]], c='blue', s=100, marker='s', label='z‚ÇÅ(1)')
        
        # True label
        true_class = np.argmax(z_val[i])
        ax.set_title(f'Sample {i+1} (True: {labels[true_class]})')
        ax.set_xlabel('Time t')
        ax.set_ylabel('z(t)')
        ax.grid(True)
        ax.legend(fontsize=8)
    
    plt.suptitle(f'z(t) Trajectories - {noise_schedule_name.title()} Noise Schedule', fontsize=16)
    plt.tight_layout()
    plt.savefig(f'{save_dir}/trajectories_{noise_schedule_name}.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. Plot 2D trajectory evolution in feature space
    print("Creating 2D trajectory evolution plot...")
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    # Select a few interesting samples to show
    sample_indices = [0, 1, 2, 3, 4, 5]
    time_indices = [0, 4, 8, 12, 16, 20]  # Show evolution at different time points
    
    for i, sample_idx in enumerate(sample_indices):
        ax = axes[i]
        
        # Plot the full trajectory in 2D space
        trajectory_2d = z_trajectories[sample_idx, :, :]  # [num_timesteps+1, 2]
        
        # Plot trajectory line
        ax.plot(trajectory_2d[:, 0], trajectory_2d[:, 1], 'k-', alpha=0.3, linewidth=1)
        
        # Plot trajectory points with color gradient
        colors = plt.cm.viridis(np.linspace(0, 1, len(trajectory_2d)))
        for j, (x, y) in enumerate(trajectory_2d):
            ax.scatter(x, y, c=[colors[j]], s=30, alpha=0.7)
        
        # Mark start and end points
        ax.scatter(trajectory_2d[0, 0], trajectory_2d[0, 1], c='red', s=200, marker='o', 
                  label='Start (t=0)', edgecolors='black', linewidth=2)
        ax.scatter(trajectory_2d[-1, 0], trajectory_2d[-1, 1], c='blue', s=200, marker='s', 
                  label='End (t=1)', edgecolors='black', linewidth=2)
        
        # Mark intermediate time points
        for t_idx in time_indices[1:-1]:  # Skip start and end
            if t_idx < len(trajectory_2d):
                ax.scatter(trajectory_2d[t_idx, 0], trajectory_2d[t_idx, 1], 
                          c='orange', s=100, marker='^', alpha=0.8)
        
        # True label
        true_class = np.argmax(z_val[sample_idx])
        ax.set_title(f'Sample {sample_idx+1} (True: {labels[true_class]})', fontsize=12)
        ax.set_xlabel('z‚ÇÄ(t)')
        ax.set_ylabel('z‚ÇÅ(t)')
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=8)
        
        # Set equal aspect ratio
        ax.set_aspect('equal', adjustable='box')
    
    plt.suptitle(f'2D Trajectory Evolution - {noise_schedule_name.title()} Noise Schedule', fontsize=16)
    plt.tight_layout()
    plt.savefig(f'{save_dir}/trajectory_evolution_{noise_schedule_name}.png', dpi=300, bbox_inches='tight')
    plt.close()


def main():
    """Main function to run the two moons example."""
    print("üåô Two Moons NoProp-CT Example")
    print("=" * 50)
    
    # Generate data
    print("Generating two moons dataset...")
    x, z = generate_two_moons_data(n_samples=1000, noise=0.1, random_state=42)
    
    # Split into train/val with random shuffling
    np.random.seed(42)
    n_samples = len(x)
    indices = np.random.permutation(n_samples)
    x_shuffled = x[indices]
    z_shuffled = z[indices]
    
    n_train = int(0.8 * n_samples)
    x_train, x_val = x_shuffled[:n_train], x_shuffled[n_train:]
    z_train, z_val = z_shuffled[:n_train], z_shuffled[n_train:]
    
    print(f"Dataset: {len(x)} samples")
    print(f"Training: {len(x_train)} samples")
    print(f"Validation: {len(x_val)} samples")
    
    # Test with learnable noise schedule
    noise_schedule_name = "learnable"
    
    print(f"\n{'='*60}")
    print(f"Training with {noise_schedule_name.upper()} noise schedule")
    print(f"{'='*60}")
    
    # Train model
    results = train_noprop_ct(
        x_train=x_train,
        z_train=z_train,
        x_val=x_val,
        z_val=z_val,
        noise_schedule_name=noise_schedule_name,
        learning_rate=1e-3,
        num_epochs=50,
        batch_size=32,
        random_seed=42
    )
    
    # Plot results
    plot_results(x_train, z_train, x_val, z_val, results)
    
    # Print final metrics
    print(f"\nFinal Results for {noise_schedule_name.upper()}:")
    print(f"  Final Train Loss: {results['train_losses'][-1]:.4f}")
    print(f"  Final Val Loss: {results['val_losses'][-1]:.4f}")
    print(f"  Final Train MSE: {results['train_mses'][-1]:.4f}")
    print(f"  Final Val MSE: {results['val_mses'][-1]:.4f}")
    print(f"  Final Train Accuracy: {results['final_train_accuracy']:.4f}")
    print(f"  Final Val Accuracy: {results['final_val_accuracy']:.4f}")
    
    print(f"\n{'='*60}")
    print("üéâ Two Moons Example Complete!")
    print("Check the 'plots' directory for visualization results.")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
