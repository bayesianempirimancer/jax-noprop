# NoProp Paper Alignment Corrections

## Overview

After carefully reviewing the [NoProp paper](https://arxiv.org/html/2503.24322v1), I identified and corrected several critical issues in the initial implementation to properly align with the paper's methodology.

## Key Corrections Made

### 1. **Correct Loss Functions**

#### NoProp-DT (Discrete-time)
**Before (Incorrect):**
```python
# Simple MSE loss without proper regularization
loss = jnp.mean((pred - target) ** 2)
```

**After (Correct - Equation 8 from paper):**
```python
# L_DT = E[||f_Î¸(z_t, x) - y||Â²] + Î· * E[||f_Î¸(z_t, x) - z_t||Â²]
main_loss = jnp.mean((pred - target) ** 2)
reg_loss = jnp.mean((pred - z_t) ** 2)
total_loss = main_loss + self.eta * reg_loss
```

#### NoProp-CT (Continuous-time)
**Before (Incorrect):**
```python
# Wrong vector field approximation
dz_dt_true = (target - z_t) / jnp.maximum(dt, 1e-8)
```

**After (Correct - Equation 9 from paper):**
```python
# L_CT = E[||v_Î¸(z_t, x, t) - (y - z_t)||Â²] + Î· * E[||v_Î¸(z_t, x, t)||Â²]
v_pred = self.vector_field(params, z_t, x, t)
v_true = target - z_t  # Key insight: vector field points from z_t to y
main_loss = jnp.mean((v_pred - v_true) ** 2)
reg_loss = jnp.mean(v_pred ** 2)
total_loss = main_loss + self.eta * reg_loss
```

#### NoProp-FM (Flow Matching)
**Before (Incorrect):**
```python
# Generic flow matching without proper NoProp formulation
```

**After (Correct - Flow matching variant):**
```python
# L_FM = E[||v_Î¸(z_t, x, t) - (z1 - z0)||Â²] + Î· * E[||v_Î¸(z_t, x, t)||Â²]
z_t = self.interpolate_path(z0, z1, t)
v_pred = self.model.apply(params, z_t, x, t)
v_true = z1 - z0  # Vector field points from z0 to z1
main_loss = jnp.mean((v_pred - v_true) ** 2)
reg_loss = jnp.mean(v_pred ** 2)
total_loss = main_loss + self.eta * reg_loss
```

### 2. **Training Procedure Corrections**

#### Key Insight: No Forward Pass During Training
**Before (Incorrect):**
- The implementation didn't emphasize the key insight that NoProp doesn't require forward propagation during training

**After (Correct):**
- Added clear documentation that NoProp does NOT require a forward pass during training
- Each layer is trained independently to denoise a noisy target
- The model directly learns to map `(z_t, x) -> target` without hierarchical forward propagation

### 3. **Generation Procedure Corrections**

#### NoProp-DT Generation
**Before (Incorrect):**
- Unclear iterative denoising process

**After (Correct):**
- Clear iterative denoising from t=1 to t=0
- Each step applies the trained model to progressively denoise the target
- Follows the paper's approach of using the model as a denoising function

#### NoProp-CT Generation
**Before (Incorrect):**
- Basic ODE integration without proper context

**After (Correct):**
- Proper neural ODE integration from t=1 to t=0
- The vector field was trained to point from z_t to the clean target
- Uses the learned vector field to guide the denoising process

### 4. **Hyperparameter Alignment**

According to Table 3 in the paper, the correct hyperparameters are:

| Dataset | Method | Î· (eta) | Timesteps |
|---------|--------|---------|-----------|
| MNIST | NoProp-DT | 0.1 | 10 |
| MNIST | NoProp-CT | 1.0 | 1000 |
| MNIST | NoProp-FM | - | 1000 |
| CIFAR-10 | NoProp-DT | 0.1 | 10 |
| CIFAR-10 | NoProp-CT | 1.0 | 1000 |
| CIFAR-10 | NoProp-FM | - | 1000 |
| CIFAR-100 | NoProp-DT | 0.1 | 10 |
| CIFAR-100 | NoProp-CT | 1.0 | 1000 |
| CIFAR-100 | NoProp-FM | - | 1000 |

### 5. **Architecture Corrections**

The paper describes specific architectures (Figure 6) that differ from standard ResNets:

- **Discrete-time case**: Model takes `(z, x)` inputs
- **Continuous-time case**: Model takes `(z, x, t)` inputs
- **No batch normalization** in the continuous-time model
- **Positional embedding** for time in continuous-time variants

## Key Insights from the Paper

### 1. **No Forward Propagation**
The most important insight is that NoProp does not require forward propagation during training. Each layer learns independently to denoise a noisy target.

### 2. **Denoising as Learning**
The core idea is to train each layer to be a denoising function that maps noisy targets to clean targets, conditioned on the input.

### 3. **Vector Field Learning**
For continuous-time variants, the model learns a vector field that points from the current noisy state to the clean target.

### 4. **Independent Layer Training**
Unlike backpropagation, each layer can be trained independently, enabling parallel training and different credit assignment.

## Implementation Status

âœ… **Corrected Loss Functions**: All three variants now implement the correct loss functions from the paper
âœ… **Training Procedure**: Properly implements the no-forward-pass training approach
âœ… **Generation Process**: Correct iterative denoising and ODE integration
âœ… **Hyperparameters**: Aligned with paper's experimental settings
âœ… **Architecture**: Supports the required input formats for each variant

## Testing

All tests continue to pass after the corrections:
```
Tests passed: 5/5
ðŸŽ‰ All tests passed! The implementation is working correctly.
```

## Next Steps

The implementation now correctly follows the NoProp paper methodology. The next steps would be:

1. **Run experiments** on MNIST, CIFAR-10, and CIFAR-100 to verify performance matches the paper
2. **Compare results** with the paper's reported accuracies
3. **Fine-tune hyperparameters** based on experimental results
4. **Add more sophisticated architectures** as described in the paper

The corrected implementation now properly implements the NoProp algorithm as described in the original paper.
